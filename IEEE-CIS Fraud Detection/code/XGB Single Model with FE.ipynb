{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis kernel is the most basic XGB model used during the competition. <br>\nThe ieee competition was my first kaggle competition and I learned a lot of knowledge and skills. <br>\nThanks to kaggler for sharing great notebooks and discussions!"},{"metadata":{},"cell_type":"markdown","source":"## Reference\n- XGB (Upvote here!) <br>\n  https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering\n  https://www.kaggle.com/xhlulu/ieee-fraud-efficient-grid-search-with-xgboost\n  \n- LGB (Upvote here!) <br>\n  https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm\n  https://www.kaggle.com/tolgahancepel/lightgbm-single-model-and-feature-engineering \n  https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n\nAs a beginner, I wanted the code to be intuitive and easy to see, and as a result I was helped by the above notebooks. :)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Load libraries and data sets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport xgboost as xgb\nimport itertools\nfrom pprint import pprint\nimport random\nimport os\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 1993\nseed_everything(SEED)\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reduce_mem_usage()\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = ['../input/ieee-fraud-detection/test_identity.csv', \n         '../input/ieee-fraud-detection/test_transaction.csv',\n         '../input/ieee-fraud-detection/train_identity.csv',\n         '../input/ieee-fraud-detection/train_transaction.csv',\n         '../input/ieee-fraud-detection/sample_submission.csv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef load_data(file):\n    return pd.read_csv(file)\n\nwith multiprocessing.Pool() as pool:\n    test_identity, test_transaction, train_identity, train_transaction, sample_submission = pool.map(load_data, files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest  = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\nsample_submission = sample_submission.set_index('TransactionID')\n\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape:\", test.shape)\n\ny = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()\n\n# Drop target, fill in NaNs\ntrain = train.drop('isFraud', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest  = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('TransactionID')\ncols_to_drop.remove('TransactionDT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Feature Engineering(FE)"},{"metadata":{},"cell_type":"markdown","source":"### Add New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def addNewFeatures(data): \n    data['uid1'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n    data['uid2'] = data['uid1'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n    \n    data['D9'] = np.where(data['D9'].isna(),0,1)\n    \n    return data\n\ntrain = addNewFeatures(train)\ntest  = addNewFeatures(test)\n\n# https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\ntrain['Transaction_hour_of_day'] = np.floor(train['TransactionDT'] / 3600) % 24\ntest['Transaction_hour_of_day'] = np.floor(test['TransactionDT'] / 3600) % 24\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n    \nfor feature in ['id_34', 'id_36']:\n    if feature in useful_features:\n        # Count encoded for both train and test\n        train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        \nfor feature in ['id_01', 'id_31', 'id_33', 'id_35', 'id_36']:\n    if feature in useful_features:\n        # Count encoded separately for train and test\n        train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n        test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering\ntrain['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\ntest['card1_count_full'] = test['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n\ntrain['card2_count_full'] = train['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\ntest['card2_count_full'] = test['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\n\ntrain['card3_count_full'] = train['card3'].map(pd.concat([train['card3'], test['card3']], ignore_index=True).value_counts(dropna=False))\ntest['card3_count_full'] = test['card3'].map(pd.concat([train['card3'], test['card3']], ignore_index=True).value_counts(dropna=False))\n\ntrain['card4_count_full'] = train['card4'].map(pd.concat([train['card4'], test['card4']], ignore_index=True).value_counts(dropna=False))\ntest['card4_count_full'] = test['card4'].map(pd.concat([train['card4'], test['card4']], ignore_index=True).value_counts(dropna=False))\n\ntrain['card5_count_full'] = train['card5'].map(pd.concat([train['card5'], test['card5']], ignore_index=True).value_counts(dropna=False))\ntest['card5_count_full'] = test['card5'].map(pd.concat([train['card5'], test['card5']], ignore_index=True).value_counts(dropna=False))\n\ntrain['card6_count_full'] = train['card6'].map(pd.concat([train['card6'], test['card6']], ignore_index=True).value_counts(dropna=False))\ntest['card6_count_full'] = test['card6'].map(pd.concat([train['card6'], test['card6']], ignore_index=True).value_counts(dropna=False))\n\n\ntrain['addr1_count_full'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\ntest['addr1_count_full'] = test['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\n\ntrain['addr2_count_full'] = train['addr2'].map(pd.concat([train['addr2'], test['addr2']], ignore_index=True).value_counts(dropna=False))\ntest['addr2_count_full'] = test['addr2'].map(pd.concat([train['addr2'], test['addr2']], ignore_index=True).value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TransactionAmt "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt']  = np.log1p(test['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TransactionDT(Set Time)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\ndef setTime(df):\n    # Temporary variables for aggregation\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n    \n    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n        \n    # Possible solo feature\n    df['is_december'] = df['DT'].dt.month\n    df['is_december'] = (df['is_december']==12).astype(np.int8)\n\n    # Holidays\n    df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n    \n    return df\n    \ntrain = setTime(train)\ntest  = setTime(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5','uid1','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train[new_col_name] = train[col].map(temp_df)\n        test[new_col_name]  = test[col].map(temp_df)\n\ntrain = train.replace(np.inf,999)\ntest  = test.replace(np.inf,999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Email Domains"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100778\ntrain['P_isproton'] = (train['P_emaildomain']=='protonmail.com')\ntrain['R_isproton'] = (train['R_emaildomain']=='protonmail.com')\ntest['P_isproton']  = (test['P_emaildomain']=='protonmail.com')\ntest['R_isproton']  = (test['R_emaildomain']=='protonmail.com')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nulls1'] = train.isna().sum(axis=1)\ntest['nulls1'] = test.isna().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin']  = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix']  = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix']  = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r]) & (df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: str(x).split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: str(x).split('.')[0])\n    \n    return df\n    \ntrain=setDomain(train)\ntest=setDomain(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Browser Version"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"]  = np.zeros(test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"] == \"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"] == \"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain = setBrowser(train)\ntest  = setBrowser(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Device Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    \n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    \n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain = setDevice(train)\ntest  = setDevice(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D8',\n          'addr1','addr2',\n          'dist1',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','device_name',\n          'id_30','id_33',\n          'uid1','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n        \n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid1']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n        \n        train[new_column] /= train[period+'_total']\n        test[new_column]  /= test[period+'_total']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_too_many_null_attr(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() / data.shape[0] > 0.9]\n    return many_null_cols\n\ndef get_too_many_repeated_val(data):\n    big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    return big_top_value_cols\n\ndef get_useless_columns(data):\n    too_many_null = get_too_many_null_attr(data)\n    print(\"More than 90% null: \" + str(len(too_many_null)))\n    too_many_repeated = get_too_many_repeated_val(data)\n    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n    cols_to_drop = list(set(too_many_null + too_many_repeated))\n    #cols_to_drop.remove('isFraud')\n    return cols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = get_useless_columns(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(cols_to_drop, axis=1)\ntest  = test.drop(cols_to_drop, axis=1)\n\nprint(train.shape)\nprint(test.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = train.select_dtypes(exclude = 'object').columns\ncategorical_cols = train.select_dtypes(include = 'object').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in train.columns:\n    if train[f].dtype.name =='object' or test[f].dtype.name =='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(-999)\ntest = test.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isnull().sum().max())\nprint(test.isnull().sum().max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['TransactionID', 'TransactionDT', 'DT'], axis=1)\nX_test  = test.drop(['TransactionID', 'TransactionDT', 'DT'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y = train['isFraud'].copy()\n\nprint(\"X_train :\", X.shape)\nprint(\"y_train :\", y.shape)\nprint(\"X_test :\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Modeling"},{"metadata":{},"cell_type":"markdown","source":"### XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"class XGBGridSearch:\n    \"\"\"\n    Source:\n    https://www.kaggle.com/xhlulu/ieee-fraud-efficient-grid-search-with-xgboost\n    \"\"\"\n    def __init__(self, param_grid, cv=3, verbose=0, \n                 shuffle=False, random_state=SEED):\n        self.param_grid = param_grid\n        self.cv = cv\n        self.random_state = random_state\n        self.verbose = verbose\n        self.shuffle = shuffle\n        \n        self.average_scores = []\n        self.scores = []\n    \n    def fit(self, X, y):\n        self._expand_params()\n        self._split_data(X, y)\n            \n        for params in tqdm(self.param_list, disable=not self.verbose):\n            avg_score, score = self._run_cv(X, y, params)\n            self.average_scores.append(avg_score)\n            self.scores.append(score)\n        \n        self._compute_best()\n\n    def _run_cv(self, X, y, params):\n        \"\"\"\n        Perform KFold CV on a single set of parameters\n        \"\"\"\n        scores = []\n        \n        for train_idx, val_idx in self.splits:\n            clf = xgb.XGBClassifier(**params)\n\n            X_train, X_val = X.iloc[train_idx, :], X.iloc[val_idx, :]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            clf.fit(X_train, y_train)\n            \n            y_val_pred = clf.predict_proba(X_val)[:, 1]\n            \n            score = roc_auc_score(y_val, y_val_pred)\n            scores.append(score)\n            \n            gc.collect()\n        \n        avg_score = sum(scores) / len(scores)\n        return avg_score, scores\n            \n    def _split_data(self, X, y):\n        kf = KFold(n_splits=self.cv, \n                   shuffle=self.shuffle, \n                   random_state=self.random_state)\n        self.splits = list(kf.split(X, y))\n            \n    def _compute_best(self):\n        \"\"\"\n        Compute best params and its corresponding score\n        \"\"\"\n        idx_best = np.argmax(self.average_scores)\n        self.best_score_ = self.average_scores[idx_best]\n        self.best_params_ = self.param_list[idx_best]\n\n    def _expand_params(self):\n        \"\"\"\n        This method expands a dictionary of lists into\n        a list of dictionaries (each dictionary is a single\n        valid params that can be input to XGBoost)\n        \"\"\"\n        keys, values = zip(*self.param_grid.items())\n        self.param_list = [\n            dict(zip(keys, v)) \n            for v in itertools.product(*values)\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [500, 1000],\n    'missing': [-999],\n    'random_state': [1993],\n    'n_jobs': [1],\n    'tree_method': ['gpu_hist'],\n    'max_depth': [9],\n    'learning_rate': [0.048, 0.05],\n    'subsample': [0.85, 0.9],\n    'colsample_bytree': [0.85, 0.9],\n    'reg_alpha': [0, 0.1],\n    'reg_lambda': [1, 0.9]\n}\n\ngrid = XGBGridSearch(param_grid, cv=6, verbose=1)\ngrid.fit(X, y)\n\nprint(\"Best Score:\", grid.best_score_)\nprint(\"Best Params:\", grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(**grid.best_params_)\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('simple_xgboost.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('.') # input argument is specified folder","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}